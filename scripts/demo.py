#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EmotionHand æ¼”ç¤ºè„šæœ¬
å®Œæ•´å±•ç¤ºä»æ•°æ®é‡‡é›†åˆ°å®æ—¶æ¨ç†çš„å…¨æµç¨‹
"""

import os
import sys
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Optional
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from feature_extraction import UnifiedFeatureExtractor
from training import ModelTrainer
from data_collection import DataCollector
from calibration import PersonalCalibrator
from real_time_inference import RealTimePipeline

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class EmotionHandDemo:
    """EmotionHand æ¼”ç¤ºç³»ç»Ÿ"""

    def __init__(self):
        print("=== EmotionHand æ¼”ç¤ºç³»ç»Ÿ ===")
        print("åŸºäºEMG+GSRçš„æƒ…ç»ªçŠ¶æ€è¯†åˆ«ç³»ç»Ÿ")
        print("å®æ—¶å»¶è¿Ÿ <100msï¼Œæ”¯æŒä¸ªæ€§åŒ–æ ¡å‡†\n")

        # åˆå§‹åŒ–ç»„ä»¶
        self.feature_extractor = UnifiedFeatureExtractor()
        self.trainer = ModelTrainer()
        self.calibrator = PersonalCalibrator()
        self.pipeline = RealTimePipeline()

        # æ¼”ç¤ºæ•°æ®
        self.demo_data = None
        self.models = {}

    def generate_demo_data(self):
        """ç”Ÿæˆæ¼”ç¤ºæ•°æ®"""
        print("ğŸ“Š ç”Ÿæˆæ¼”ç¤ºæ•°æ®...")

        # ç”ŸæˆEMGæ•°æ® (8é€šé“)
        sample_rate_emg = 1000
        duration = 10  # 10ç§’æ•°æ®
        n_samples = sample_rate_emg * duration

        # åˆ›å»ºä¸åŒçŠ¶æ€çš„æ¨¡æ‹Ÿä¿¡å·
        t = np.linspace(0, duration, n_samples)
        emg_data = np.zeros((n_samples, 8))

        # ä¸ºæ¯ä¸ªé€šé“æ·»åŠ ä¸åŒé¢‘ç‡çš„ä¿¡å·
        frequencies = [20, 50, 80, 120, 200, 300, 350, 400]
        amplitudes = [0.5, 0.3, 0.2, 0.4, 0.3, 0.2, 0.1, 0.1]

        for i in range(8):
            # åŸºç¡€ä¿¡å·
            signal = amplitudes[i] * np.sin(2 * np.pi * frequencies[i] * t)

            # æ·»åŠ å™ªå£°
            signal += 0.1 * np.random.randn(n_samples)

            # æ·»åŠ è°ƒåˆ¶ (æ¨¡æ‹Ÿè‚Œè‚‰æ”¶ç¼©)
            if i < 4:  # å‰4ä¸ªé€šé“æœ‰æ›´å¼ºçš„æ´»åŠ¨
                envelope = 0.5 + 0.5 * np.sin(2 * np.pi * 0.5 * t)
                signal *= envelope

            emg_data[:, i] = signal

        # ç”ŸæˆGSRæ•°æ® (1é€šé“)
        sample_rate_gsr = 100
        n_samples_gsr = sample_rate_gsr * duration
        t_gsr = np.linspace(0, duration, n_samples_gsr)

        # GSRä¿¡å· (ä½é¢‘)
        gsr_data = 0.1 * np.sin(2 * np.pi * 0.1 * t_gsr)  # 0.1HzåŸºç¡€ä¿¡å·
        gsr_data += 0.05 * np.sin(2 * np.pi * 0.05 * t_gsr)  # 0.05Hzæ…¢å˜ä¿¡å·
        gsr_data += 0.02 * np.random.randn(n_samples_gsr)  # å™ªå£°

        # ç”Ÿæˆæ ‡ç­¾
        n_windows = 150  # 150ä¸ªæ—¶é—´çª—å£
        gesture_labels = np.random.choice(['Fist', 'Open', 'Pinch', 'Point'], n_windows)
        state_labels = np.random.choice(['Relaxed', 'Focused', 'Stressed', 'Fatigued'], n_windows)

        self.demo_data = {
            'emg_data': emg_data,
            'gsr_data': gsr_data,
            'gesture_labels': gesture_labels,
            'state_labels': state_labels,
            'sample_rate_emg': sample_rate_emg,
            'sample_rate_gsr': sample_rate_gsr
        }

        print(f"âœ… æ¼”ç¤ºæ•°æ®ç”Ÿæˆå®Œæˆ:")
        print(f"   EMGæ•°æ®: {emg_data.shape} (æ—¶é•¿: {duration}ç§’)")
        print(f"   GSRæ•°æ®: {gsr_data.shape}")
        print(f"   æ‰‹åŠ¿æ ‡ç­¾: {np.unique(gesture_labels)}")
        print(f"   çŠ¶æ€æ ‡ç­¾: {np.unique(state_labels)}")

    def extract_demo_features(self):
        """æå–æ¼”ç¤ºç‰¹å¾"""
        if self.demo_data is None:
            self.generate_demo_data()

        print("\nğŸ”§ æå–ç‰¹å¾...")

        # æå–ç»„åˆç‰¹å¾
        features, emg_windows, gsr_windows = self.feature_extractor.extract_combined_features(
            self.demo_data['emg_data'],
            self.demo_data['gsr_data']
        )

        print(f"âœ… ç‰¹å¾æå–å®Œæˆ:")
        print(f"   ç‰¹å¾çŸ©é˜µ: {features.shape}")
        print(f"   EMGçª—å£æ•°: {len(emg_windows)}")
        print(f"   GSRçª—å£æ•°: {len(gsr_windows)}")
        print(f"   ç‰¹å¾ç»´åº¦: {features.shape[1]}")

        return features

    def train_demo_models(self, features):
        """è®­ç»ƒæ¼”ç¤ºæ¨¡å‹"""
        print("\nğŸ§  è®­ç»ƒæ¨¡å‹...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        n_samples = min(len(features), len(self.demo_data['gesture_labels']))
        X = features[:n_samples]
        y_gesture = self.demo_data['gesture_labels'][:n_samples]
        y_state = self.demo_data['state_labels'][:n_samples]

        # è®­ç»ƒæ‰‹åŠ¿åˆ†ç±»å™¨
        print("è®­ç»ƒæ‰‹åŠ¿åˆ†ç±»å™¨...")
        gesture_model, gesture_metadata = self.trainer.train_model(
            X, y_gesture, model_type='lightgbm', mode='gesture'
        )
        self.models['gesture'] = gesture_model

        print(f"âœ… æ‰‹åŠ¿æ¨¡å‹è®­ç»ƒå®Œæˆ - æµ‹è¯•å‡†ç¡®ç‡: {gesture_metadata['test_score']:.3f}")

        # è®­ç»ƒçŠ¶æ€åˆ†ç±»å™¨
        print("è®­ç»ƒçŠ¶æ€åˆ†ç±»å™¨...")
        state_model, state_metadata = self.trainer.train_model(
            X, y_state, model_type='lightgbm', mode='state'
        )
        self.models['state'] = state_model

        print(f"âœ… çŠ¶æ€æ¨¡å‹è®­ç»ƒå®Œæˆ - æµ‹è¯•å‡†ç¡®ç‡: {state_metadata['test_score']:.3f}")

        # ä¿å­˜æ¨¡å‹
        os.makedirs('./models', exist_ok=True)
        import joblib
        joblib.dump(gesture_model, './models/demo_gesture_model.joblib')
        joblib.dump(state_model, './models/demo_state_model.joblib')
        joblib.dump(self.trainer.scalers['gesture'], './models/demo_scaler.joblib')

        print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° ./models/")

    def simulate_real_time_inference(self):
        """æ¨¡æ‹Ÿå®æ—¶æ¨ç†"""
        print("\nâš¡ æ¨¡æ‹Ÿå®æ—¶æ¨ç†...")

        if self.demo_data is None:
            self.generate_demo_data()

        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®æµ
        def simulate_data_stream():
            """æ¨¡æ‹Ÿæ•°æ®æµç”Ÿæˆå™¨"""
            emg_buffer = []
            gsr_buffer = []

            for i in range(100):  # æ¨¡æ‹Ÿ100ä¸ªæ—¶é—´æ­¥
                # è·å–ä¸€å°æ®µæ•°æ®
                start_idx = i * 10
                end_idx = start_idx + 256

                if end_idx < len(self.demo_data['emg_data']):
                    emg_segment = self.demo_data['emg_data'][start_idx:end_idx]
                    gsr_segment = self.demo_data['gsr_data'][start_idx//10:end_idx//10]

                    yield emg_segment, gsr_segment, i

                time.sleep(0.01)  # æ¨¡æ‹Ÿå®æ—¶æ€§

        # æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
        predictions = []
        latencies = []

        for emg_segment, gsr_segment, step in simulate_data_stream():
            start_time = time.time()

            # ç‰¹å¾æå–
            features, _, _ = self.feature_extractor.extract_combined_features(
                emg_segment, gsr_segment
            )

            if features.shape[0] > 0:
                feature_vec = features[-1]  # å–æœ€æ–°ç‰¹å¾

                # æ‰‹åŠ¿é¢„æµ‹
                if 'gesture' in self.models:
                    gesture_pred = self.models['gesture'].predict([feature_vec])[0]
                    gesture_conf = np.max(self.models['gesture'].predict_proba([feature_vec])[0])
                else:
                    gesture_pred = "Unknown"
                    gesture_conf = 0.0

                # çŠ¶æ€é¢„æµ‹
                if 'state' in self.models:
                    state_pred = self.models['state'].predict([feature_vec])[0]
                    state_conf = np.max(self.models['state'].predict_proba([feature_vec])[0])
                else:
                    state_pred = "Unknown"
                    state_conf = 0.0

                # è®¡ç®—å»¶è¿Ÿ
                latency = (time.time() - start_time) * 1000  # ms

                predictions.append({
                    'step': step,
                    'gesture': gesture_pred,
                    'state': state_pred,
                    'confidence': min(gesture_conf, state_conf),
                    'latency': latency
                })

                latencies.append(latency)

                # å®æ—¶æ˜¾ç¤º
                if step % 10 == 0:
                    print(f"æ­¥éª¤ {step:3d}: {gesture_pred:8s} | {state_pred:8s} | "
                          f"ç½®ä¿¡åº¦: {min(gesture_conf, state_conf):.3f} | "
                          f"å»¶è¿Ÿ: {latency:.1f}ms")

        # ç»Ÿè®¡ç»“æœ
        if predictions:
            avg_latency = np.mean(latencies)
            max_latency = np.max(latencies)
            confidence_threshold = 0.6
            high_conf_count = sum(1 for p in predictions if p['confidence'] >= confidence_threshold)

            print(f"\nğŸ“Š å®æ—¶æ¨ç†ç»Ÿè®¡:")
            print(f"   æ€»é¢„æµ‹æ¬¡æ•°: {len(predictions)}")
            print(f"   å¹³å‡å»¶è¿Ÿ: {avg_latency:.1f}ms")
            print(f"   æœ€å¤§å»¶è¿Ÿ: {max_latency:.1f}ms")
            print(f"   é«˜ç½®ä¿¡åº¦é¢„æµ‹(>={confidence_threshold}): {high_conf_count}/{len(predictions)}")
            print(f"   å®æ—¶æ€§èƒ½: {'âœ…è¾¾æ ‡' if avg_latency < 100 else 'âŒè¶…æ ‡'} (ç›®æ ‡: <100ms)")

            return predictions

    def visualize_demo_results(self, predictions):
        """å¯è§†åŒ–æ¼”ç¤ºç»“æœ"""
        if not predictions:
            return

        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('EmotionHand æ¼”ç¤ºç»“æœ', fontsize=16)

        # 1. é¢„æµ‹ç»“æœæ—¶é—´çº¿
        ax = axes[0, 0]
        steps = [p['step'] for p in predictions]
        gestures = [p['gesture'] for p in predictions]
        states = [p['state'] for p in predictions]

        # æ‰‹åŠ¿æ—¶é—´çº¿
        unique_gestures = list(set(gestures))
        gesture_colors = plt.cm.Set3(np.linspace(0, 1, len(unique_gestures)))
        gesture_dict = {g: gesture_colors[i] for i, g in enumerate(unique_gestures)}

        for i, (step, gesture) in enumerate(zip(steps, gestures)):
            ax.scatter(step, i % 5, c=[gesture_dict[gesture]], s=20, alpha=0.7)

        ax.set_title('æ‰‹åŠ¿è¯†åˆ«æ—¶é—´çº¿')
        ax.set_xlabel('æ—¶é—´æ­¥')
        ax.set_ylabel('é¢„æµ‹ä½ç½®')

        # æ·»åŠ å›¾ä¾‹
        for gesture, color in gesture_dict.items():
            ax.scatter([], [], c=[color], label=gesture, s=50)
        ax.legend()

        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒ
        ax = axes[0, 1]
        confidences = [p['confidence'] for p in predictions]
        ax.hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax.axvline(x=0.6, color='red', linestyle='--', label='ç½®ä¿¡åº¦é˜ˆå€¼')
        ax.set_title('é¢„æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ')
        ax.set_xlabel('ç½®ä¿¡åº¦')
        ax.set_ylabel('é¢‘æ¬¡')
        ax.legend()

        # 3. å»¶è¿Ÿåˆ†æ
        ax = axes[1, 0]
        latencies = [p['latency'] for p in predictions]
        ax.plot(latencies, alpha=0.7, color='green')
        ax.axhline(y=100, color='red', linestyle='--', label='100msç›®æ ‡')
        ax.set_title('å®æ—¶å»¶è¿Ÿåˆ†æ')
        ax.set_xlabel('é¢„æµ‹åºå·')
        ax.set_ylabel('å»¶è¿Ÿ (ms)')
        ax.legend()

        # 4. çŠ¶æ€åˆ†å¸ƒé¥¼å›¾
        ax = axes[1, 1]
        state_counts = {}
        for p in predictions:
            state_counts[p['state']] = state_counts.get(p['state'], 0) + 1

        colors = plt.cm.Set2(np.linspace(0, 1, len(state_counts)))
        ax.pie(state_counts.values(), labels=state_counts.keys(), autopct='%1.1f%%',
               colors=colors, startangle=90)
        ax.set_title('çŠ¶æ€è¯†åˆ«åˆ†å¸ƒ')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        output_dir = './docs/results'
        os.makedirs(output_dir, exist_ok=True)
        plot_path = os.path.join(output_dir, f'demo_results_{int(time.time())}.png')
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜: {plot_path}")

        plt.show()

    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹å®Œæ•´æ¼”ç¤ºæµç¨‹\n")

        try:
            # æ­¥éª¤1: ç”Ÿæˆæ¼”ç¤ºæ•°æ®
            self.generate_demo_data()

            # æ­¥éª¤2: ç‰¹å¾æå–
            features = self.extract_demo_features()

            # æ­¥éª¤3: è®­ç»ƒæ¨¡å‹
            self.train_demo_models(features)

            # æ­¥éª¤4: æ¨¡æ‹Ÿå®æ—¶æ¨ç†
            predictions = self.simulate_real_time_inference()

            # æ­¥éª¤5: å¯è§†åŒ–ç»“æœ
            self.visualize_demo_results(predictions)

            print("\nğŸ‰ å®Œæ•´æ¼”ç¤ºæµç¨‹å®Œæˆ!")
            print("\nğŸ“‹ æ¼”ç¤ºæ€»ç»“:")
            print("   âœ… æ•°æ®ç”Ÿæˆ: 10ç§’EMG+GSRæ¨¡æ‹Ÿæ•°æ®")
            print("   âœ… ç‰¹å¾æå–: LibEMGé£æ ¼çš„å¤šç»´ç‰¹å¾")
            print("   âœ… æ¨¡å‹è®­ç»ƒ: LightGBMæ‰‹åŠ¿+çŠ¶æ€åˆ†ç±»å™¨")
            print("   âœ… å®æ—¶æ¨ç†: <100mså»¶è¿Ÿçš„é«˜æ€§èƒ½æ¨ç†")
            print("   âœ… ç»“æœå¯è§†åŒ–: å¤šç»´åº¦åˆ†æå›¾è¡¨")

        except Exception as e:
            logger.error(f"æ¼”ç¤ºæµç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def interactive_demo(self):
        """äº¤äº’å¼æ¼”ç¤º"""
        print("=== EmotionHand äº¤äº’å¼æ¼”ç¤º ===\n")

        while True:
            print("ğŸ“‹ æ¼”ç¤ºèœå•:")
            print("1. è¿è¡Œå®Œæ•´æ¼”ç¤º")
            print("2. ä»…ç”Ÿæˆæ¼”ç¤ºæ•°æ®")
            print("3. ä»…è®­ç»ƒæ¨¡å‹")
            print("4. ä»…æ¨¡æ‹Ÿå®æ—¶æ¨ç†")
            print("5. æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
            print("6. é€€å‡º")

            choice = input("\nè¯·é€‰æ‹©æ¼”ç¤ºé¡¹ç›® (1-6): ").strip()

            if choice == '1':
                self.run_complete_demo()
            elif choice == '2':
                self.generate_demo_data()
                features = self.extract_demo_features()
                print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {features.shape}")
            elif choice == '3':
                if self.demo_data is None:
                    self.generate_demo_data()
                features = self.extract_demo_features()
                self.train_demo_models(features)
            elif choice == '4':
                if not self.models:
                    print("âš ï¸ éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹ï¼Œè¯·é€‰æ‹©é€‰é¡¹3")
                else:
                    self.simulate_real_time_inference()
            elif choice == '5':
                print("\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
                print(f"   æ¼”ç¤ºæ•°æ®: {'å·²ç”Ÿæˆ' if self.demo_data else 'æœªç”Ÿæˆ'}")
                print(f"   è®­ç»ƒæ¨¡å‹: {list(self.models.keys())}")
                print(f"   ç‰¹å¾æå–å™¨: å·²å°±ç»ª")
                print(f"   æ•°æ®é‡‡é›†å™¨: å·²å°±ç»ª")
                print(f"   æ ¡å‡†å™¨: å·²å°±ç»ª")
                print(f"   å®æ—¶ç®¡çº¿: å·²å°±ç»ª")
            elif choice == '6':
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨EmotionHandæ¼”ç¤ºç³»ç»Ÿ!")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

            input("\næŒ‰å›è½¦ç»§ç»­...")

def main():
    """ä¸»å‡½æ•°"""
    demo = EmotionHandDemo()

    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        if sys.argv[1] == '--full':
            demo.run_complete_demo()
        elif sys.argv[1] == '--interactive':
            demo.interactive_demo()
        else:
            print("ç”¨æ³•:")
            print("  python demo.py --full         # è¿è¡Œå®Œæ•´æ¼”ç¤º")
            print("  python demo.py --interactive  # äº¤äº’å¼æ¼”ç¤º")
            print("  python demo.py                # äº¤äº’å¼æ¼”ç¤º")
    else:
        demo.interactive_demo()

if __name__ == "__main__":
    main()